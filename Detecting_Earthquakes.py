# -*- coding: utf-8 -*-
"""ML#3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18zFciJDzWN354r9AOlCH0IE3A-Wr6cdK

# **Processing the data**
"""

#libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#loading the data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)

# get the 'index' column
data.drop('index', axis=1, inplace=True)

# showing the data
print(data.head())

# Show basic information of descriptive statistics
print(data.info())

"""# **descriptive statistics and plots**"""

# Show basic information of the file
print(data.describe())

# Visualizing features distribution
for column in data.columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Density')
    plt.show()

"""# **handling with outliers**"""

# Importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest

# Loading data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
indices = data['index']  # Store the index column
data.drop('index', axis=1, inplace=True)

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Applying PCA for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)

# Using Isolation Forest to detect outliers
clf = IsolationForest(contamination=0.05)  # adjust contamination if needed
preds = clf.fit_predict(scaled_data)

# Plotting
plt.figure(figsize=(10, 6))

# Normal data points
plt.scatter(pca_data[preds == 1, 0], pca_data[preds == 1, 1], s=5, alpha=0.6, label='Normal Data')

# Outliers
plt.scatter(pca_data[preds == -1, 0], pca_data[preds == -1, 1], s=5, alpha=0.6, color='green', label='Outliers')

plt.title('PCA of Measurements Outliers')
plt.xlabel(' Component 1')
plt.ylabel(' Component 2')
plt.legend(loc='upper right')
plt.show()

"""# **getting the most far outliers**"""

# Loading data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
indices = data['index']  # Store the index column
data.drop('index', axis=1, inplace=True)

# Dict to store features
outliers_dict = {}

# Calculating outliers
for column in data.columns:
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1

    # Defining bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Finding outliers
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)][column]

    # Finding  outliers
    if not outliers.empty:
        min_outlier = outliers.min()
        max_outlier = outliers.max()
        outliers_dict[column] = {'Min Outlier': min_outlier, 'Max Outlier': max_outlier}

# Displaying  outliers
outliers_df = pd.DataFrame(outliers_dict).transpose()
print(outliers_df)

"""# **creating outliers list**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import LocalOutlierFactor

# Loading data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
indices = data['index']  # Store the index column
data.drop('index', axis=1, inplace=True)

# Standardizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Using Local Outlier Factor to detect outliers
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)  # adjust parameters if needed
preds = lof.fit_predict(scaled_data)

# Filtering the data to only the outliers
outliers = data[preds == -1]

# Identifying the minimum and maximum outlier for each feature
outliers_min = outliers.min()
outliers_max = outliers.max()

# Creating a DataFrame to display min and max outliers per feature
outliers_summary = pd.DataFrame({
    'Feature': data.columns,
    'Min Outlier': outliers_min.values,
    'Max Outlier': outliers_max.values
})

print(outliers_summary)

"""# **calibration of features**"""

#  generating calibration display
for feature in [f"feature_{i}" for i in range(18)]:
    X = data[[feature]]

    # Spliting the data for evaluation
    X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

    # Applying IsolationForest to get anomaly scores
    clf = IsolationForest(contamination=0.05)
    clf.fit(X_train)
    anomaly_scores = clf.decision_function(X_test)

    # Converting anomaly scores into pseudo-probabilities: lower score means more likely to be an outlier
    pseudo_probs = 1 - (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min())

    # Assuming points in the lowest 5% of scores are outliers, creating true binary labels
    y_true = (pseudo_probs < 0.05).astype(int)

    # Calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, pseudo_probs, n_bins=10)

    plt.figure(figsize=(8, 8))
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", label=f"IsolationForest on {feature}")
    plt.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title(f'Calibration Curve for {feature}')
    plt.legend()
    plt.show()

"""# **making earthquakes prediction**"""

# Applying Isolation Forest  detection
clf = IsolationForest(contamination=0.01)  # Assuming approx 1% of the data points are anomalies
clf.fit(data)
predictions = clf.predict(data)

#  anomalies
anomaly_indices = indices[predictions == -1].values
print(f" predicting earthquakes: {list(anomaly_indices)}")

"""# **getting best parameters **"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import f1_score, make_scorer
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Loading the data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
X = data.drop('index', axis=1)

# Featuring scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Outlier Detection (like before)
iso_forest = IsolationForest(contamination=0.01, random_state=42)
outliers = iso_forest.fit_predict(X_scaled)

# Converting -1 (outlier) to 1 and 1 (inlier) to 0
y_pseudo = [1 if label == -1 else 0 for label in outliers]

# Modeling Selection & Hyperparameter Tuning
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42)
}

param_grid = {
    'RandomForest': {
        'n_estimators': [50, 100, 150],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10]
    },
    'GradientBoosting': {
        'n_estimators': [50, 100, 150],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7]
    },
    'SVM': {
        'C': [0.1, 1, 10],
        'gamma': ['scale', 'auto', 0.01, 0.1]
    }
}

best_f1_score = 0
best_params = None
best_model_name = None

for model_name, model in models.items():
    grid_search = GridSearchCV(model, param_grid[model_name], cv=StratifiedKFold(n_splits=5),
                               scoring=make_scorer(f1_score), verbose=2, n_jobs=-1)
    grid_search.fit(X_scaled, y_pseudo)

    if grid_search.best_score_ > best_f1_score:
        best_f1_score = grid_search.best_score_
        best_params = grid_search.best_params_
        best_model_name = model_name

print(f"Best Model: {best_model_name}")
print(f"Best F-1 Score: {best_f1_score:.2f}")
print(f"Best Parameters: {best_params}")

"""# **Random Forest**"""

import pandas as pd
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split

# Loading the data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
indices = data['index']
data.drop('index', axis=1, inplace=True)

# Step 1: Fitting the Data
# Train a random forest on a subset of data
# and predict the remaining data to get an anomaly score.
X_train, X_test = train_test_split(data, test_size=0.5, random_state=44)

# Using regressor because we're not dealing with a classification task
# and we want to get a continuous output to determine outliers.
model = RandomForestRegressor(n_estimators=100, random_state=44)
model.fit(X_train, X_train)
predicted = model.predict(X_test)

# Step 2: Outlier Score
# Calculating mean squared error as the anomaly score
mse = ((X_test - predicted) ** 2).mean(axis=1)

# Step 3: Thresholding
# If the mean squared error is greater than a certain threshold,
# we consider that point as an outlier.
# Here, we're using the 99th percentile as the threshold.
threshold = np.percentile(mse, 99)
outliers = X_test.index[mse > threshold]

print(f"Indices predicting earthquakes using Random Forest: {list(outliers)}")

"""# **F-1 Score**"""

from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, make_scorer
from sklearn.preprocessing import StandardScaler

# Loading the data
data_path = 'A3_data.csv'
data = pd.read_csv(data_path)
X = data.drop('index', axis=1)

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA for dimensionality reduction
pca = PCA(n_components=5)  # Reducing to 5 dimensions, but this can be adjusted
X_pca = pca.fit_transform(X_scaled)

# Using Isolation Forest on the PCA-transformed data to detect outliers
iso_forest = IsolationForest(contamination=0.01, random_state=44)
outliers = iso_forest.fit_predict(X_pca)

# Converting -1 (outlier) to 1 and 1 (inlier) to 0 to create pseudo-labels
y_pseudo = [1 if label == -1 else 0 for label in outliers]

# Training a classifier (RandomForest in this case) on the PCA-transformed data
clf = RandomForestClassifier(random_state=42)
cv = StratifiedKFold(n_splits=5)
f1_scores = cross_val_score(clf, X_pca, y_pseudo, cv=cv, scoring=make_scorer(f1_score), n_jobs=-1)

print(f"Average F-1 Score using PCA + pseudo-labels: {np.mean(f1_scores):.2f}")

"""# **most useful features**"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Sampling dataset generation
np.random.seed(42)
data = pd.DataFrame(np.random.rand(2584, 18), columns=[f"feature_{i}" for i in range(18)])
data["earthquake"] = np.random.randint(0, 2, size=2584)  # Assuming a binary outcome for simplicity
data["index"] = range(2584)

# Assuming 'earthquake' is the target variable
X = data.drop(["earthquake", "index"], axis=1)  # Drop the target and the 'index' column
y = data["earthquake"]

# Initializing and train a RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X, y)

# Extracting feature importance
feature_importance = clf.feature_importances_

# Mapping features to their importance
features = list(X.columns)
feature_map = {feature: importance for feature, importance in zip(features, feature_importance)}

# Sorting features based on importance
sorted_features = sorted(feature_map.items(), key=lambda x: x[1], reverse=True)

# Displaying the top 5 features
print("Top 5 most important features:")
for i, (feature, importance) in enumerate(sorted_features[:5]):
    print(f"{i+1}. {feature} - Importance: {importance:.4f}")